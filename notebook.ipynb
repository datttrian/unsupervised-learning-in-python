{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unsupervised Learning in Python\n",
                "\n",
                "## Clustering for Dataset Exploration\n",
                "\n",
                "### Clustering 2D points\n",
                "\n",
                "From the scatter plot of the previous exercise, you saw that the points\n",
                "seem to separate into 3 clusters. You'll now create a KMeans model to\n",
                "find 3 clusters, and fit it to the data points from the previous\n",
                "exercise. After the model has been fit, you'll obtain the cluster labels\n",
                "for some new points using the `.predict()` method.\n",
                "\n",
                "You are given the array `points` from the previous exercise, and also an\n",
                "array `new_points`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `KMeans` from `sklearn.cluster`.\n",
                "- Using `KMeans()`, create a `KMeans` instance called `model` to find\n",
                "  `3` clusters. To specify the number of clusters, use the `n_clusters`\n",
                "  keyword argument.\n",
                "- Use the `.fit()` method of `model` to fit the model to the array of\n",
                "  points `points`.\n",
                "- Use the `.predict()` method of `model` to predict the cluster labels\n",
                "  of `new_points`, assigning the result to `labels`.\n",
                "- Hit submit to see the cluster labels of `new_points`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import KMeans\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# Create a KMeans instance with 3 clusters: model\n",
                "model = KMeans(n_clusters=3)\n",
                "\n",
                "# Fit model to points\n",
                "model.fit(points)\n",
                "\n",
                "# Determine the cluster labels of new_points: labels\n",
                "labels = model.predict(new_points)\n",
                "\n",
                "# Print cluster labels of new_points\n",
                "print(labels)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inspect your clustering\n",
                "\n",
                "Let's now inspect the clustering you performed in the previous exercise!\n",
                "\n",
                "A solution to the previous exercise has already run, so `new_points` is\n",
                "an array of points and `labels` is the array of their cluster labels.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `matplotlib.pyplot` as `plt`.\n",
                "- Assign column `0` of `new_points` to `xs`, and column `1` of\n",
                "  `new_points` to `ys`.\n",
                "- Make a scatter plot of `xs` and `ys`, specifying the `c=labels`\n",
                "  keyword arguments to color the points by their cluster label. Also\n",
                "  specify `alpha=0.5`.\n",
                "- Compute the coordinates of the centroids using the `.cluster_centers_`\n",
                "  attribute of `model`.\n",
                "- Assign column `0` of `centroids` to `centroids_x`, and column `1` of\n",
                "  `centroids` to `centroids_y`.\n",
                "- Make a scatter plot of `centroids_x` and `centroids_y`, using `'D'` (a\n",
                "  diamond) as a marker by specifying the `marker` parameter. Set the\n",
                "  size of the markers to be `50` using `s=50`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pyplot\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "# Assign the columns of new_points: xs and ys\n",
                "xs = new_points[:,0]\n",
                "ys = new_points[:,1]\n",
                "\n",
                "# Make a scatter plot of xs and ys, using labels to define the colors\n",
                "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
                "\n",
                "# Assign the cluster centers: centroids\n",
                "centroids = model.cluster_centers_\n",
                "\n",
                "# Assign the columns of centroids: centroids_x, centroids_y\n",
                "centroids_x = centroids[:,0]\n",
                "centroids_y = centroids[:,1]\n",
                "\n",
                "# Make a scatter plot of centroids_x and centroids_y\n",
                "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How many clusters of grain?\n",
                "\n",
                "In the video, you learned how to choose a good number of clusters for a\n",
                "dataset using the k-means inertia graph. You are given an array\n",
                "`samples` containing the measurements (such as area, perimeter, length,\n",
                "and several others) of samples of grain. What's a good number of\n",
                "clusters in this case?\n",
                "\n",
                "`KMeans` and PyPlot (`plt`) have already been imported for you.\n",
                "\n",
                "This dataset was sourced from the [UCI Machine Learning\n",
                "Repository](https://archive.ics.uci.edu/ml/datasets/seeds).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- For each of the given values of `k`, perform the following steps:\n",
                "- Create a `KMeans` instance called `model` with `k` clusters.\n",
                "- Fit the model to the grain data `samples`.\n",
                "- Append the value of the `inertia_` attribute of `model` to the list\n",
                "  `inertias`.\n",
                "- The code to plot `ks` vs `inertias` has been written for you, so hit\n",
                "  submit to see the plot!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ks = range(1, 6)\n",
                "inertias = []\n",
                "\n",
                "for k in ks:\n",
                "    # Create a KMeans instance with k clusters: model\n",
                "    model = KMeans(n_clusters=k)\n",
                "    \n",
                "    # Fit model to samples\n",
                "    model.fit(samples)\n",
                "    \n",
                "    # Append the inertia to the list of inertias\n",
                "    inertias.append(model.inertia_)\n",
                "    \n",
                "# Plot ks vs inertias\n",
                "plt.plot(ks, inertias, '-o')\n",
                "plt.xlabel('number of clusters, k')\n",
                "plt.ylabel('inertia')\n",
                "plt.xticks(ks)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluating the grain clustering\n",
                "\n",
                "In the previous exercise, you observed from the inertia plot that 3 is a\n",
                "good number of clusters for the grain data. In fact, the grain samples\n",
                "come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and\n",
                "\"Canadian\". In this exercise, cluster the grain samples into three\n",
                "clusters, and compare the clusters to the grain varieties using a\n",
                "cross-tabulation.\n",
                "\n",
                "You have the array `samples` of grain samples, and a list `varieties`\n",
                "giving the grain variety for each sample. Pandas (`pd`) and `KMeans`\n",
                "have already been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create a `KMeans` model called `model` with `3` clusters.\n",
                "- Use the `.fit_predict()` method of `model` to fit it to `samples` and\n",
                "  derive the cluster labels. Using `.fit_predict()` is the same as using\n",
                "  `.fit()` followed by `.predict()`.\n",
                "- Create a DataFrame `df` with two columns named `'labels'` and\n",
                "  `'varieties'`, using `labels` and `varieties`, respectively, for the\n",
                "  column values. This has been done for you.\n",
                "- Use the `pd.crosstab()` function on `df['labels']` and\n",
                "  `df['varieties']` to count the number of times each grain variety\n",
                "  coincides with each cluster label. Assign the result to `ct`.\n",
                "- Hit submit to see the cross-tabulation!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a KMeans model with 3 clusters: model\n",
                "model = KMeans(n_clusters=3)\n",
                "\n",
                "# Use fit_predict to fit model and obtain cluster labels: labels\n",
                "labels = model.fit_predict(samples)\n",
                "\n",
                "# Create a DataFrame with clusters and varieties as columns: df\n",
                "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
                "\n",
                "# Create crosstab: ct\n",
                "ct = pd.crosstab(df['labels'], df['varieties'])\n",
                "\n",
                "# Display ct\n",
                "print(ct)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scaling fish data for clustering\n",
                "\n",
                "You are given an array `samples` giving measurements of fish. Each row\n",
                "represents an individual fish. The measurements, such as weight in\n",
                "grams, length in centimeters, and the percentage ratio of height to\n",
                "length, have very different scales. In order to cluster this data\n",
                "effectively, you'll need to standardize these features first. In this\n",
                "exercise, you'll build a pipeline to standardize and cluster the data.\n",
                "\n",
                "These fish measurement data were sourced from the [Journal of Statistics\n",
                "Education](http://ww2.amstat.org/publications/jse/jse_data_archive.htm).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `make_pipeline` from `sklearn.pipeline`.\n",
                "  - `StandardScaler` from `sklearn.preprocessing`.\n",
                "  - `KMeans` from `sklearn.cluster`.\n",
                "- Create an instance of `StandardScaler` called `scaler`.\n",
                "- Create an instance of `KMeans` with `4` clusters called `kmeans`.\n",
                "- Create a pipeline called `pipeline` that chains `scaler` and `kmeans`.\n",
                "  To do this, you just need to pass them in as arguments to\n",
                "  `make_pipeline()`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# Create scaler: scaler\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# Create KMeans instance: kmeans\n",
                "kmeans = KMeans(n_clusters=4)\n",
                "\n",
                "# Create pipeline: pipeline\n",
                "pipeline = make_pipeline(scaler, kmeans)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clustering the fish data\n",
                "\n",
                "You'll now use your standardization and clustering pipeline from the\n",
                "previous exercise to cluster the fish by their measurements, and then\n",
                "create a cross-tabulation to compare the cluster labels with the fish\n",
                "species.\n",
                "\n",
                "As before, `samples` is the 2D array of fish measurements. Your pipeline\n",
                "is available as `pipeline`, and the species of every fish sample is\n",
                "given by the list `species`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Fit the pipeline to the fish measurements `samples`.\n",
                "- Obtain the cluster labels for `samples` by using the `.predict()`\n",
                "  method of `pipeline`.\n",
                "- Using `pd.DataFrame()`, create a DataFrame `df` with two columns named\n",
                "  `'labels'` and `'species'`, using `labels` and `species`,\n",
                "  respectively, for the column values.\n",
                "- Using `pd.crosstab()`, create a cross-tabulation `ct` of\n",
                "  `df['labels']` and `df['species']`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Fit the pipeline to samples\n",
                "pipeline.fit(samples)\n",
                "\n",
                "# Calculate the cluster labels: labels\n",
                "labels = pipeline.predict(samples)\n",
                "\n",
                "# Create a DataFrame with labels and species as columns: df\n",
                "df = pd.DataFrame({'labels': labels, 'species': species})\n",
                "\n",
                "# Create crosstab: ct\n",
                "ct = pd.crosstab(df['labels'], df['species'])\n",
                "\n",
                "# Display ct\n",
                "print(ct)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clustering stocks using KMeans\n",
                "\n",
                "In this exercise, you'll cluster companies using their daily stock price\n",
                "movements (i.e. the dollar difference between the closing and opening\n",
                "prices for each trading day). You are given a NumPy array `movements` of\n",
                "daily price movements from 2010 to 2015 (obtained from Yahoo! Finance),\n",
                "where each row corresponds to a company, and each column corresponds to\n",
                "a trading day.\n",
                "\n",
                "Some stocks are more expensive than others. To account for this, include\n",
                "a `Normalizer` at the beginning of your pipeline. The Normalizer will\n",
                "separately transform each company's stock price to a relative scale\n",
                "before the clustering begins.\n",
                "\n",
                "Note that `Normalizer()` is different to `StandardScaler()`, which you\n",
                "used in the previous exercise. While `StandardScaler()` standardizes\n",
                "**features** (such as the features of the fish data from the previous\n",
                "exercise) by removing the mean and scaling to unit variance,\n",
                "`Normalizer()` rescales **each sample** - here, each company's stock\n",
                "price - independently of the other.\n",
                "\n",
                "`KMeans` and `make_pipeline` have already been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `Normalizer` from `sklearn.preprocessing`.\n",
                "- Create an instance of `Normalizer` called `normalizer`.\n",
                "- Create an instance of `KMeans` called `kmeans` with `10` clusters.\n",
                "- Using `make_pipeline()`, create a pipeline called `pipeline` that\n",
                "  chains `normalizer` and `kmeans`.\n",
                "- Fit the pipeline to the `movements` array.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Normalizer\n",
                "from sklearn.preprocessing import Normalizer\n",
                "\n",
                "# Create a normalizer: normalizer\n",
                "normalizer = Normalizer()\n",
                "\n",
                "# Create a KMeans model with 10 clusters: kmeans\n",
                "kmeans = KMeans(n_clusters=10)\n",
                "\n",
                "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
                "pipeline = make_pipeline(normalizer, kmeans)\n",
                "\n",
                "# Fit pipeline to the daily price movements\n",
                "pipeline.fit(movements)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Which stocks move together?\n",
                "\n",
                "In the previous exercise, you clustered companies by their daily stock\n",
                "price movements. So which company have stock prices that tend to change\n",
                "in the same way? You'll now inspect the cluster labels from your\n",
                "clustering to find out.\n",
                "\n",
                "Your solution to the previous exercise has already been run. Recall that\n",
                "you constructed a Pipeline `pipeline` containing a `KMeans` model and\n",
                "fit it to the NumPy array `movements` of daily stock movements. In\n",
                "addition, a list `companies` of the company names is available.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Use the `.predict()` method of the pipeline to predict the labels for\n",
                "  `movements`.\n",
                "- Align the cluster labels with the list of company names `companies` by\n",
                "  creating a DataFrame `df` with `labels` and `companies` as columns.\n",
                "  This has been done for you.\n",
                "- Use the `.sort_values()` method of `df` to sort the DataFrame by the\n",
                "  `'labels'` column, and print the result.\n",
                "- Hit submit and take a moment to see which companies are together in\n",
                "  each cluster!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Predict the cluster labels: labels\n",
                "labels = pipeline.predict(movements)\n",
                "\n",
                "# Create a DataFrame aligning labels and companies: df\n",
                "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
                "\n",
                "# Display df sorted by cluster label\n",
                "print(df.sort_values('labels'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization with Hierarchical Clustering and t-SNE\n",
                "\n",
                "### Hierarchical clustering of the grain data\n",
                "\n",
                "In the video, you learned that the SciPy `linkage()` function performs\n",
                "hierarchical clustering on an array of samples. Use the `linkage()`\n",
                "function to obtain a hierarchical clustering of the grain samples, and\n",
                "use `dendrogram()` to visualize the result. A sample of the grain\n",
                "measurements is provided in the array `samples`, while the variety of\n",
                "each grain sample is given by the list `varieties`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `linkage` and `dendrogram` from `scipy.cluster.hierarchy`.\n",
                "  - `matplotlib.pyplot` as `plt`.\n",
                "- Perform hierarchical clustering on `samples` using the `linkage()`\n",
                "  function with the `method='complete'` keyword argument. Assign the\n",
                "  result to `mergings`.\n",
                "- Plot a dendrogram using the `dendrogram()` function on `mergings`.\n",
                "  Specify the keyword arguments `labels=varieties`, `leaf_rotation=90`,\n",
                "  and `leaf_font_size=6`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "from scipy.cluster.hierarchy import linkage, dendrogram\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Calculate the linkage: mergings\n",
                "mergings = linkage(samples, method='complete')\n",
                "\n",
                "# Plot the dendrogram, using varieties as labels\n",
                "dendrogram(mergings,\n",
                "           labels=varieties,\n",
                "           leaf_rotation=90,\n",
                "           leaf_font_size=6,\n",
                ")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hierarchies of stocks\n",
                "\n",
                "In chapter 1, you used k-means clustering to cluster companies according\n",
                "to their stock price movements. Now, you'll perform hierarchical\n",
                "clustering of the companies. You are given a NumPy array of price\n",
                "movements `movements`, where the rows correspond to companies, and a\n",
                "list of the company names `companies`. SciPy hierarchical clustering\n",
                "doesn't fit into a sklearn pipeline, so you'll need to use the\n",
                "`normalize()` function from `sklearn.preprocessing` instead of\n",
                "`Normalizer`.\n",
                "\n",
                "`linkage` and `dendrogram` have already been imported from\n",
                "`scipy.cluster.hierarchy`, and PyPlot has been imported as `plt`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `normalize` from `sklearn.preprocessing`.\n",
                "- Rescale the price movements for each stock by using the `normalize()`\n",
                "  function on `movements`.\n",
                "- Apply the `linkage()` function to `normalized_movements`, using\n",
                "  `'complete'` linkage, to calculate the hierarchical clustering. Assign\n",
                "  the result to `mergings`.\n",
                "- Plot a dendrogram of the hierarchical clustering, using the list\n",
                "  `companies` of company names as the `labels`. In addition, specify the\n",
                "  `leaf_rotation=90`, and `leaf_font_size=6` keyword arguments as you\n",
                "  did in the previous exercise.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import normalize\n",
                "from sklearn.preprocessing import normalize\n",
                "\n",
                "# Normalize the movements: normalized_movements\n",
                "normalized_movements = normalize(movements)\n",
                "\n",
                "# Calculate the linkage: mergings\n",
                "mergings = linkage(normalized_movements, method='complete')\n",
                "\n",
                "# Plot the dendrogram\n",
                "dendrogram(\n",
                "    mergings,\n",
                "    labels=companies,\n",
                "    leaf_rotation=90,\n",
                "    leaf_font_size=6\n",
                ")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Different linkage, different hierarchical clustering!\n",
                "\n",
                "In the video, you saw a hierarchical clustering of the voting countries\n",
                "at the Eurovision song contest using `'complete'` linkage. Now, perform\n",
                "a hierarchical clustering of the voting countries with `'single'`\n",
                "linkage, and compare the resulting dendrogram with the one in the video.\n",
                "Different linkage, different hierarchical clustering!\n",
                "\n",
                "You are given an array `samples`. Each row corresponds to a voting\n",
                "country, and each column corresponds to a performance that was voted\n",
                "for. The list `country_names` gives the name of each voting country.\n",
                "This dataset was obtained from\n",
                "[Eurovision](https://www.eurovision.tv/page/results).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `linkage` and `dendrogram` from `scipy.cluster.hierarchy`.\n",
                "- Perform hierarchical clustering on `samples` using the `linkage()`\n",
                "  function with the `method='single'` keyword argument. Assign the\n",
                "  result to `mergings`.\n",
                "- Plot a dendrogram of the hierarchical clustering, using the list\n",
                "  `country_names` as the `labels`. In addition, specify the\n",
                "  `leaf_rotation=90`, and `leaf_font_size=6` keyword arguments as you\n",
                "  have done earlier.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.cluster.hierarchy import linkage, dendrogram\n",
                "\n",
                "# Calculate the linkage: mergings\n",
                "mergings = linkage(samples, method='single')\n",
                "\n",
                "# Plot the dendrogram\n",
                "dendrogram(mergings,\n",
                "           labels=country_names,\n",
                "           leaf_rotation=90,\n",
                "           leaf_font_size=6,\n",
                ")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Extracting the cluster labels\n",
                "\n",
                "In the previous exercise, you saw that the intermediate clustering of\n",
                "the grain samples at height 6 has 3 clusters. Now, use the `fcluster()`\n",
                "function to extract the cluster labels for this intermediate clustering,\n",
                "and compare the labels with the grain varieties using a\n",
                "cross-tabulation.\n",
                "\n",
                "The hierarchical clustering has already been performed and `mergings` is\n",
                "the result of the `linkage()` function. The list `varieties` gives the\n",
                "variety of each grain sample.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `pandas` as `pd`.\n",
                "  - `fcluster` from `scipy.cluster.hierarchy`.\n",
                "- Perform a flat hierarchical clustering by using the `fcluster()`\n",
                "  function on `mergings`. Specify a maximum height of `6` and the\n",
                "  keyword argument `criterion='distance'`.\n",
                "- Create a DataFrame `df` with two columns named `'labels'` and\n",
                "  `'varieties'`, using `labels` and `varieties`, respectively, for the\n",
                "  column values. This has been done for you.\n",
                "- Create a cross-tabulation `ct` between `df['labels']` and\n",
                "  `df['varieties']` to count the number of times each grain variety\n",
                "  coincides with each cluster label.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "import pandas as pd\n",
                "from scipy.cluster.hierarchy import fcluster\n",
                "\n",
                "# Use fcluster to extract labels: labels\n",
                "labels = fcluster(mergings, 6, criterion='distance')\n",
                "\n",
                "# Create a DataFrame with labels and varieties as columns: df\n",
                "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
                "\n",
                "# Create crosstab: ct\n",
                "ct = pd.crosstab(df['labels'], df['varieties'])\n",
                "\n",
                "# Display ct\n",
                "print(ct)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### t-SNE visualization of grain dataset\n",
                "\n",
                "In the video, you saw t-SNE applied to the iris dataset. In this\n",
                "exercise, you'll apply t-SNE to the grain samples data and inspect the\n",
                "resulting t-SNE features using a scatter plot. You are given an array\n",
                "`samples` of grain samples and a list `variety_numbers` giving the\n",
                "variety number of each grain sample.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `TSNE` from `sklearn.manifold`.\n",
                "- Create a TSNE instance called `model` with `learning_rate=200`.\n",
                "- Apply the `.fit_transform()` method of `model` to `samples`. Assign\n",
                "  the result to `tsne_features`.\n",
                "- Select the column `0` of `tsne_features`. Assign the result to `xs`.\n",
                "- Select the column `1` of `tsne_features`. Assign the result to `ys`.\n",
                "- Make a scatter plot of the t-SNE features `xs` and `ys`. To color the\n",
                "  points by the grain variety, specify the additional keyword argument\n",
                "  `c=variety_numbers`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import TSNE\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "# Create a TSNE instance: model\n",
                "model = TSNE(learning_rate=200)\n",
                "\n",
                "# Apply fit_transform to samples: tsne_features\n",
                "tsne_features = model.fit_transform(samples)\n",
                "\n",
                "# Select the 0th feature: xs\n",
                "xs = tsne_features[:,0]\n",
                "\n",
                "# Select the 1st feature: ys\n",
                "ys = tsne_features[:,1]\n",
                "\n",
                "# Scatter plot, coloring by variety_numbers\n",
                "plt.scatter(xs, ys, c=variety_numbers)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A t-SNE map of the stock market\n",
                "\n",
                "t-SNE provides great visualizations when the individual samples can be\n",
                "labeled. In this exercise, you'll apply t-SNE to the company stock price\n",
                "data. A scatter plot of the resulting t-SNE features, labeled by the\n",
                "company names, gives you a map of the stock market! The stock price\n",
                "movements for each company are available as the array\n",
                "`normalized_movements` (these have already been normalized for you). The\n",
                "list `companies` gives the name of each company. PyPlot (`plt`) has been\n",
                "imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `TSNE` from `sklearn.manifold`.\n",
                "- Create a TSNE instance called `model` with `learning_rate=50`.\n",
                "- Apply the `.fit_transform()` method of `model` to\n",
                "  `normalized_movements`. Assign the result to `tsne_features`.\n",
                "- Select column `0` and column `1` of `tsne_features`.\n",
                "- Make a scatter plot of the t-SNE features `xs` and `ys`. Specify the\n",
                "  additional keyword argument `alpha=0.5`.\n",
                "- Code to label each point with its company name has been written for\n",
                "  you using `plt.annotate()`, so just hit submit to see the\n",
                "  visualization!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import TSNE\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "# Create a TSNE instance: model\n",
                "model = TSNE(learning_rate=50)\n",
                "\n",
                "# Apply fit_transform to normalized_movements: tsne_features\n",
                "tsne_features = model.fit_transform(normalized_movements)\n",
                "\n",
                "# Select the 0th feature: xs\n",
                "xs = tsne_features[:,0]\n",
                "\n",
                "# Select the 1th feature: ys\n",
                "ys = tsne_features[:,1]\n",
                "\n",
                "# Scatter plot\n",
                "plt.scatter(xs, ys, alpha=0.5)\n",
                "\n",
                "# Annotate the points\n",
                "for x, y, company in zip(xs, ys, companies):\n",
                "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Decorrelating Your Data and Dimension Reduction\n",
                "\n",
                "### Correlated data in nature\n",
                "\n",
                "You are given an array `grains` giving the width and length of samples\n",
                "of grain. You suspect that width and length will be correlated. To\n",
                "confirm this, make a scatter plot of width vs length and measure their\n",
                "Pearson correlation.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `matplotlib.pyplot` as `plt`.\n",
                "  - `pearsonr` from `scipy.stats`.\n",
                "- Assign column `0` of `grains` to `width` and column `1` of `grains` to\n",
                "  `length`.\n",
                "- Make a scatter plot with `width` on the x-axis and `length` on the\n",
                "  y-axis.\n",
                "- Use the `pearsonr()` function to calculate the Pearson correlation of\n",
                "  `width` and `length`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.stats import pearsonr\n",
                "\n",
                "# Assign the 0th column of grains: width\n",
                "width = grains[:,0]\n",
                "\n",
                "# Assign the 1st column of grains: length\n",
                "length = grains[:,1]\n",
                "\n",
                "# Scatter plot width vs length\n",
                "plt.scatter(width, length)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "# Calculate the Pearson correlation\n",
                "correlation, pvalue = pearsonr(width, length)\n",
                "\n",
                "# Display the correlation\n",
                "print(correlation)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decorrelating the grain measurements with PCA\n",
                "\n",
                "You observed in the previous exercise that the width and length\n",
                "measurements of the grain are correlated. Now, you'll use PCA to\n",
                "decorrelate these measurements, then plot the decorrelated points and\n",
                "measure their Pearson correlation.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `PCA` from `sklearn.decomposition`.\n",
                "- Create an instance of `PCA` called `model`.\n",
                "- Use the `.fit_transform()` method of `model` to apply the PCA\n",
                "  transformation to `grains`. Assign the result to `pca_features`.\n",
                "- The subsequent code to extract, plot, and compute the Pearson\n",
                "  correlation of the first two columns `pca_features` has been written\n",
                "  for you, so hit submit to see the result!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import PCA\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Create PCA instance: model\n",
                "model = PCA()\n",
                "\n",
                "# Apply the fit_transform method of model to grains: pca_features\n",
                "pca_features = model.fit_transform(grains)\n",
                "\n",
                "# Assign 0th column of pca_features: xs\n",
                "xs = pca_features[:,0]\n",
                "\n",
                "# Assign 1st column of pca_features: ys\n",
                "ys = pca_features[:,1]\n",
                "\n",
                "# Scatter plot xs vs ys\n",
                "plt.scatter(xs, ys)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "# Calculate the Pearson correlation of xs and ys\n",
                "correlation, pvalue = pearsonr(xs, ys)\n",
                "\n",
                "# Display the correlation\n",
                "print(correlation)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The first principal component\n",
                "\n",
                "The first principal component of the data is the direction in which the\n",
                "data varies the most. In this exercise, your job is to use PCA to find\n",
                "the first principal component of the length and width measurements of\n",
                "the grain samples, and represent it as an arrow on the scatter plot.\n",
                "\n",
                "The array `grains` gives the length and width of the grain samples.\n",
                "PyPlot (`plt`) and `PCA` have already been imported for you.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Make a scatter plot of the grain measurements. This has been done for\n",
                "  you.\n",
                "- Create a `PCA` instance called `model`.\n",
                "- Fit the model to the `grains` data.\n",
                "- Extract the coordinates of the mean of the data using the `.mean_`\n",
                "  attribute of `model`.\n",
                "- Get the first principal component of `model` using the\n",
                "  `.components_[0,:]` attribute.\n",
                "- Plot the first principal component as an arrow on the scatter plot,\n",
                "  using the `plt.arrow()` function. You have to specify the first two\n",
                "  arguments - `mean[0]` and `mean[1]`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make a scatter plot of the untransformed points\n",
                "plt.scatter(grains[:,0], grains[:,1])\n",
                "\n",
                "# Create a PCA instance: model\n",
                "model = PCA()\n",
                "\n",
                "# Fit model to points\n",
                "model.fit(grains)\n",
                "\n",
                "# Get the mean of the grain samples: mean\n",
                "mean = model.mean_\n",
                "\n",
                "# Get the first principal component: first_pc\n",
                "first_pc = model.components_[0,:]\n",
                "\n",
                "# Plot first_pc as an arrow, starting at mean\n",
                "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
                "\n",
                "# Keep axes on same scale\n",
                "plt.axis('equal')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variance of the PCA features\n",
                "\n",
                "The fish dataset is 6-dimensional. But what is its *intrinsic*\n",
                "dimension? Make a plot of the variances of the PCA features to find out.\n",
                "As before, `samples` is a 2D array, where each row represents a fish.\n",
                "You'll need to standardize the features first.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Create an instance of `StandardScaler` called `scaler`.\n",
                "- Create a `PCA` instance called `pca`.\n",
                "- Use the `make_pipeline()` function to create a pipeline chaining\n",
                "  `scaler` and `pca`.\n",
                "- Use the `.fit()` method of `pipeline` to fit it to the fish samples\n",
                "  `samples`.\n",
                "- Extract the number of components used using the `.n_components_`\n",
                "  attribute of `pca`. Place this inside a `range()` function and store\n",
                "  the result as `features`.\n",
                "- Use the `plt.bar()` function to plot the explained variances, with\n",
                "  `features` on the x-axis and `pca.explained_variance_` on the y-axis.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.pipeline import make_pipeline\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create scaler: scaler\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# Create a PCA instance: pca\n",
                "pca = PCA()\n",
                "\n",
                "# Create pipeline: pipeline\n",
                "pipeline = make_pipeline(scaler, pca)\n",
                "\n",
                "# Fit the pipeline to 'samples'\n",
                "pipeline.fit(samples)\n",
                "\n",
                "# Plot the explained variances\n",
                "features = range(pca.n_components_)\n",
                "plt.bar(features, pca.explained_variance_)\n",
                "plt.xlabel('PCA feature')\n",
                "plt.ylabel('variance')\n",
                "plt.xticks(features)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dimension reduction of the fish measurements\n",
                "\n",
                "In a previous exercise, you saw that `2` was a reasonable choice for the\n",
                "\"intrinsic dimension\" of the fish measurements. Now use PCA for\n",
                "dimensionality reduction of the fish measurements, retaining only the 2\n",
                "most important components.\n",
                "\n",
                "The fish measurements have already been scaled for you, and are\n",
                "available as `scaled_samples`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `PCA` from `sklearn.decomposition`.\n",
                "- Create a PCA instance called `pca` with `n_components=2`.\n",
                "- Use the `.fit()` method of `pca` to fit it to the scaled fish\n",
                "  measurements `scaled_samples`.\n",
                "- Use the `.transform()` method of `pca` to transform the\n",
                "  `scaled_samples`. Assign the result to `pca_features`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import PCA\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Create a PCA instance with 2 components: pca\n",
                "pca = PCA(n_components=2)\n",
                "\n",
                "# Fit the PCA instance to the scaled samples\n",
                "pca.fit(scaled_samples)\n",
                "\n",
                "# Transform the scaled samples: pca_features\n",
                "pca_features = pca.transform(scaled_samples)\n",
                "\n",
                "# Print the shape of pca_features\n",
                "print(pca_features.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A tf-idf word-frequency array\n",
                "\n",
                "In this exercise, you'll create a tf-idf word frequency array for a toy\n",
                "collection of documents. For this, use the `TfidfVectorizer` from\n",
                "sklearn. It transforms a list of documents into a word frequency array,\n",
                "which it outputs as a csr_matrix. It has `fit()` and `transform()`\n",
                "methods like other sklearn objects.\n",
                "\n",
                "You are given a list `documents` of toy documents about pets. Its\n",
                "contents have been printed in the IPython Shell.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
                "- Create a `TfidfVectorizer` instance called `tfidf`.\n",
                "- Apply `.fit_transform()` method of `tfidf` to `documents` and assign\n",
                "  the result to `csr_mat`. This is a word-frequency array in csr_matrix\n",
                "  format.\n",
                "- Inspect `csr_mat` by calling its `.toarray()` method and printing the\n",
                "  result. This has been done for you.\n",
                "- The columns of the array correspond to words. Get the list of words by\n",
                "  calling the `.get_feature_names()` method of `tfidf`, and assign the\n",
                "  result to `words`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import TfidfVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "# Create a TfidfVectorizer: tfidf\n",
                "tfidf = TfidfVectorizer() \n",
                "\n",
                "# Apply fit_transform to document: csr_mat\n",
                "csr_mat = tfidf.fit_transform(documents)\n",
                "\n",
                "# Print result of toarray() method\n",
                "print(csr_mat.toarray())\n",
                "\n",
                "# Get the words: words\n",
                "words = tfidf.get_feature_names()\n",
                "\n",
                "# Print words\n",
                "print(words)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clustering Wikipedia part I\n",
                "\n",
                "You saw in the video that `TruncatedSVD` is able to perform PCA on\n",
                "sparse arrays in csr_matrix format, such as word-frequency arrays.\n",
                "Combine your knowledge of TruncatedSVD and k-means to cluster some\n",
                "popular pages from Wikipedia. In this exercise, build the pipeline. In\n",
                "the next exercise, you'll apply it to the word-frequency array of some\n",
                "Wikipedia articles.\n",
                "\n",
                "Create a Pipeline object consisting of a TruncatedSVD followed by\n",
                "KMeans. (This time, we've precomputed the word-frequency matrix for you,\n",
                "so there's no need for a TfidfVectorizer).\n",
                "\n",
                "The Wikipedia dataset you will be working with was obtained from\n",
                "[here](https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `TruncatedSVD` from `sklearn.decomposition`.\n",
                "  - `KMeans` from `sklearn.cluster`.\n",
                "  - `make_pipeline` from `sklearn.pipeline`.\n",
                "- Create a `TruncatedSVD` instance called `svd` with `n_components=50`.\n",
                "- Create a `KMeans` instance called `kmeans` with `n_clusters=6`.\n",
                "- Create a pipeline called `pipeline` consisting of `svd` and `kmeans`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.pipeline import make_pipeline\n",
                "\n",
                "# Create a TruncatedSVD instance: svd\n",
                "svd = TruncatedSVD(n_components=50)\n",
                "\n",
                "# Create a KMeans instance: kmeans\n",
                "kmeans = KMeans(n_clusters=6)\n",
                "\n",
                "# Create a pipeline: pipeline\n",
                "pipeline = make_pipeline(svd, kmeans)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clustering Wikipedia part II\n",
                "\n",
                "It is now time to put your pipeline from the previous exercise to work!\n",
                "You are given an array `articles` of tf-idf word-frequencies of some\n",
                "popular Wikipedia articles, and a list `titles` of their titles. Use\n",
                "your pipeline to cluster the Wikipedia articles.\n",
                "\n",
                "A solution to the previous exercise has been pre-loaded for you, so a\n",
                "Pipeline `pipeline` chaining TruncatedSVD with KMeans is available.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Fit the pipeline to the word-frequency array `articles`.\n",
                "- Predict the cluster labels.\n",
                "- Align the cluster labels with the list `titles` of article titles by\n",
                "  creating a DataFrame `df` with `labels` and `titles` as columns. This\n",
                "  has been done for you.\n",
                "- Use the `.sort_values()` method of `df` to sort the DataFrame by the\n",
                "  `'label'` column, and print the result.\n",
                "- Hit submit and take a moment to investigate your amazing clustering of\n",
                "  Wikipedia pages!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Fit the pipeline to articles\n",
                "pipeline.fit(articles)\n",
                "\n",
                "# Calculate the cluster labels: labels\n",
                "labels = pipeline.predict(articles)\n",
                "\n",
                "# Create a DataFrame aligning labels and titles: df\n",
                "df = pd.DataFrame({'label': labels, 'article': titles})\n",
                "\n",
                "# Display df sorted by cluster label\n",
                "print(df.sort_values('label'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Discovering Interpretable Features\n",
                "\n",
                "### NMF applied to Wikipedia articles\n",
                "\n",
                "In the video, you saw NMF applied to transform a toy word-frequency\n",
                "array. Now it's your turn to apply NMF, this time using the tf-idf\n",
                "word-frequency array of Wikipedia articles, given as a csr matrix\n",
                "`articles`. Here, fit the model and transform the articles. In the next\n",
                "exercise, you'll explore the result.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `NMF` from `sklearn.decomposition`.\n",
                "- Create an `NMF` instance called `model` with `6` components.\n",
                "- Fit the model to the word count data `articles`.\n",
                "- Use the `.transform()` method of `model` to transform `articles`, and\n",
                "  assign the result to `nmf_features`.\n",
                "- Print `nmf_features` to get a first idea what it looks like\n",
                "  (`.round(2)` rounds the entries to 2 decimal places.)\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import NMF\n",
                "from sklearn.decomposition import NMF\n",
                "\n",
                "# Create an NMF instance: model\n",
                "model = NMF(n_components=6)\n",
                "\n",
                "# Fit the model to articles\n",
                "model.fit(articles)\n",
                "\n",
                "# Transform the articles: nmf_features\n",
                "nmf_features = model.transform(articles)\n",
                "\n",
                "# Print the NMF features\n",
                "print(nmf_features.round(2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NMF features of the Wikipedia articles\n",
                "\n",
                "Now you will explore the NMF features you created in the previous\n",
                "exercise. A solution to the previous exercise has been pre-loaded, so\n",
                "the array `nmf_features` is available. Also available is a list `titles`\n",
                "giving the title of each Wikipedia article.\n",
                "\n",
                "When investigating the features, notice that for both actors, the NMF\n",
                "feature 3 has by far the highest value. This means that both articles\n",
                "are reconstructed using mainly the 3rd NMF component. In the next video,\n",
                "you'll see why: NMF components represent topics (for instance, acting!).\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Create a DataFrame `df` from `nmf_features` using `pd.DataFrame()`.\n",
                "  Set the index to `titles` using `index=titles`.\n",
                "- Use the `.loc[]` accessor of `df` to select the row with title\n",
                "  `'Anne Hathaway'`, and print the result. These are the NMF features\n",
                "  for the article about the actress Anne Hathaway.\n",
                "- Repeat the last step for `'Denzel Washington'` (another actor).\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Create a pandas DataFrame: df\n",
                "df = pd.DataFrame(nmf_features, index=titles)\n",
                "\n",
                "# Print the row for 'Anne Hathaway'\n",
                "print(df.loc['Anne Hathaway'])\n",
                "\n",
                "# Print the row for 'Denzel Washington'\n",
                "print(df.loc['Denzel Washington'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NMF learns topics of documents\n",
                "\n",
                "In the video, you learned when NMF is applied to documents, the\n",
                "components correspond to topics of documents, and the NMF features\n",
                "reconstruct the documents from the topics. Verify this for yourself for\n",
                "the NMF model that you built earlier using the Wikipedia articles.\n",
                "Previously, you saw that the 3rd NMF feature value was high for the\n",
                "articles about actors Anne Hathaway and Denzel Washington. In this\n",
                "exercise, identify the topic of the corresponding NMF component.\n",
                "\n",
                "The NMF model you built earlier is available as `model`, while `words`\n",
                "is a list of the words that label the columns of the word-frequency\n",
                "array.\n",
                "\n",
                "After you are done, take a moment to recognize the topic that the\n",
                "articles about Anne Hathaway and Denzel Washington have in common!\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Create a DataFrame `components_df` from `model.components_`, setting\n",
                "  `columns=words` so that columns are labeled by the words.\n",
                "- Print `components_df.shape` to check the dimensions of the DataFrame.\n",
                "- Use the `.iloc[]` accessor on the DataFrame `components_df` to select\n",
                "  row `3`. Assign the result to `component`.\n",
                "- Call the `.nlargest()` method of `component`, and print the result.\n",
                "  This gives the five words with the highest values for that component.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Create a DataFrame: components_df\n",
                "components_df = pd.DataFrame(model.components_, columns=words)\n",
                "\n",
                "# Print the shape of the DataFrame\n",
                "print(components_df.shape)\n",
                "\n",
                "# Select row 3: component\n",
                "component = components_df.iloc[3]\n",
                "\n",
                "# Print result of nlargest\n",
                "print(component.nlargest())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explore the LED digits dataset\n",
                "\n",
                "In the following exercises, you'll use NMF to decompose grayscale images\n",
                "into their commonly occurring patterns. Firstly, explore the image\n",
                "dataset and see how it is encoded as an array. You are given 100 images\n",
                "as a 2D array `samples`, where each row represents a single 13x8 image.\n",
                "The images in your dataset are pictures of a LED digital display.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `matplotlib.pyplot` as `plt`.\n",
                "- Select row `0` of `samples` and assign the result to `digit`. For\n",
                "  example, to select column `2` of an array `a`, you could use `a[:,2]`.\n",
                "  Remember that since `samples` is a NumPy array, you can't use the\n",
                "  `.loc[]` or `iloc[]` accessors to select specific rows or columns.\n",
                "- Print `digit`. This has been done for you. Notice that it is a 1D\n",
                "  array of 0s and 1s.\n",
                "- Use the `.reshape()` method of `digit` to get a 2D array with shape\n",
                "  `(13, 8)`. Assign the result to `bitmap`.\n",
                "- Print `bitmap`, and notice that the 1s show the digit 7!\n",
                "- Use the `plt.imshow()` function to display `bitmap` as an image.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pyplot\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "# Select the 0th row: digit\n",
                "digit = samples[0,:]\n",
                "\n",
                "# Print digit\n",
                "print(digit)\n",
                "\n",
                "# Reshape digit to a 13x8 array: bitmap\n",
                "bitmap = digit.reshape((13, 8))\n",
                "\n",
                "# Print bitmap\n",
                "print(bitmap)\n",
                "\n",
                "# Use plt.imshow to display bitmap\n",
                "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
                "plt.colorbar()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NMF learns the parts of images\n",
                "\n",
                "Now use what you've learned about NMF to decompose the digits dataset.\n",
                "You are again given the digit images as a 2D array `samples`. This time,\n",
                "you are also provided with a function `show_as_image()` that displays\n",
                "the image encoded by any 1D array:\n",
                "\n",
                "    def show_as_image(sample):\n",
                "        bitmap = sample.reshape((13, 8))\n",
                "        plt.figure()\n",
                "        plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
                "        plt.colorbar()\n",
                "        plt.show()\n",
                "\n",
                "After you are done, take a moment to look through the plots and notice\n",
                "how NMF has expressed the digit as a sum of the components!\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `NMF` from `sklearn.decomposition`.\n",
                "- Create an `NMF` instance called `model` with `7` components. (7 is the\n",
                "  number of cells in an LED display).\n",
                "- Apply the `.fit_transform()` method of `model` to `samples`. Assign\n",
                "  the result to `features`.\n",
                "- To each component of the model (accessed via `model.components_`),\n",
                "  apply the `show_as_image()` function to that component inside the\n",
                "  loop.\n",
                "- Assign the row `0` of `features` to `digit_features`.\n",
                "- Print `digit_features`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import NMF\n",
                "from sklearn.decomposition import NMF\n",
                "\n",
                "# Create an NMF model: model\n",
                "model = NMF(n_components=7)\n",
                "\n",
                "# Apply fit_transform to samples: features\n",
                "features = model.fit_transform(samples)\n",
                "\n",
                "# Call show_as_image on each component\n",
                "for component in model.components_:\n",
                "    show_as_image(component)\n",
                "\n",
                "# Select the 0th row of features: digit_features\n",
                "digit_features = features[0,:]\n",
                "\n",
                "# Print digit_features\n",
                "print(digit_features)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PCA doesn't learn parts\n",
                "\n",
                "Unlike NMF, PCA *doesn't* learn the parts of things. Its components do\n",
                "not correspond to topics (in the case of documents) or to parts of\n",
                "images, when trained on images. Verify this for yourself by inspecting\n",
                "the components of a PCA model fit to the dataset of LED digit images\n",
                "from the previous exercise. The images are available as a 2D array\n",
                "`samples`. Also available is a modified version of the `show_as_image()`\n",
                "function which colors a pixel red if the value is negative.\n",
                "\n",
                "After submitting the answer, notice that the components of PCA do not\n",
                "represent meaningful parts of images of LED digits!\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `PCA` from `sklearn.decomposition`.\n",
                "- Create a `PCA` instance called `model` with `7` components.\n",
                "- Apply the `.fit_transform()` method of `model` to `samples`. Assign\n",
                "  the result to `features`.\n",
                "- To each component of the model (accessed via `model.components_`),\n",
                "  apply the `show_as_image()` function to that component inside the\n",
                "  loop.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import PCA\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Create a PCA instance: model\n",
                "model = PCA(n_components=7)\n",
                "\n",
                "# Apply fit_transform to samples: features\n",
                "features = model.fit_transform(samples)\n",
                "\n",
                "# Call show_as_image on each component\n",
                "for component in model.components_:\n",
                "    show_as_image(component)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Which articles are similar to 'Cristiano Ronaldo'?\n",
                "\n",
                "In the video, you learned how to use NMF features and the cosine\n",
                "similarity to find similar articles. Apply this to your NMF model for\n",
                "popular Wikipedia articles, by finding the articles most similar to the\n",
                "article about the footballer Cristiano Ronaldo. The NMF features you\n",
                "obtained earlier are available as `nmf_features`, while `titles` is a\n",
                "list of the article titles.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `normalize` from `sklearn.preprocessing`.\n",
                "- Apply the `normalize()` function to `nmf_features`. Store the result\n",
                "  as `norm_features`.\n",
                "- Create a DataFrame `df` from `norm_features`, using `titles` as an\n",
                "  index.\n",
                "- Use the `.loc[]` accessor of `df` to select the row of\n",
                "  `'Cristiano Ronaldo'`. Assign the result to `article`.\n",
                "- Apply the `.dot()` method of `df` to `article` to calculate the cosine\n",
                "  similarity of every row with `article`.\n",
                "- Print the result of the `.nlargest()` method of `similarities` to\n",
                "  display the most similar articles. This has been done for you, so hit\n",
                "  'Submit Answer' to see the result!\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import normalize\n",
                "\n",
                "# Normalize the NMF features: norm_features\n",
                "norm_features = normalize(nmf_features)\n",
                "\n",
                "# Create a DataFrame: df\n",
                "df = pd.DataFrame(norm_features, index=titles)\n",
                "\n",
                "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
                "article = df.loc['Cristiano Ronaldo']\n",
                "\n",
                "# Compute the dot products: similarities\n",
                "similarities = df.dot(article)\n",
                "\n",
                "# Display those with the largest cosine similarity\n",
                "print(similarities.nlargest())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Recommend musical artists part I\n",
                "\n",
                "In this exercise and the next, you'll use what you've learned about NMF\n",
                "to recommend popular music artists! You are given a sparse array\n",
                "`artists` whose rows correspond to artists and whose columns correspond\n",
                "to users. The entries give the number of times each artist was listened\n",
                "to by each user.\n",
                "\n",
                "In this exercise, build a pipeline and transform the array into\n",
                "normalized NMF features. The first step in the pipeline, `MaxAbsScaler`,\n",
                "transforms the data so that all users have the same influence on the\n",
                "model, regardless of how many different artists they've listened to. In\n",
                "the next exercise, you'll use the resulting normalized NMF features for\n",
                "recommendation!\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import:\n",
                "  - `NMF` from `sklearn.decomposition`.\n",
                "  - `Normalizer` and `MaxAbsScaler` from `sklearn.preprocessing`.\n",
                "  - `make_pipeline` from `sklearn.pipeline`.\n",
                "- Create an instance of `MaxAbsScaler` called `scaler`.\n",
                "- Create an `NMF` instance with `20` components called `nmf`.\n",
                "- Create an instance of `Normalizer` called `normalizer`.\n",
                "- Create a pipeline called `pipeline` that chains together `scaler`,\n",
                "  `nmf`, and `normalizer`.\n",
                "- Apply the `.fit_transform()` method of `pipeline` to `artists`. Assign\n",
                "  the result to `norm_features`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform the necessary imports\n",
                "from sklearn.decomposition import NMF\n",
                "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
                "from sklearn.pipeline import make_pipeline\n",
                "\n",
                "# Create a MaxAbsScaler: scaler\n",
                "scaler = MaxAbsScaler()\n",
                "\n",
                "# Create an NMF model: nmf\n",
                "nmf = NMF(n_components=20)\n",
                "\n",
                "# Create a Normalizer: normalizer\n",
                "normalizer = Normalizer()\n",
                "\n",
                "# Create a pipeline: pipeline\n",
                "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
                "\n",
                "# Apply fit_transform to artists: norm_features\n",
                "norm_features = pipeline.fit_transform(artists)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Recommend musical artists part II\n",
                "\n",
                "Suppose you were a big fan of Bruce Springsteen - which other musical\n",
                "artists might you like? Use your NMF features from the previous exercise\n",
                "and the cosine similarity to find similar musical artists. A solution to\n",
                "the previous exercise has been run, so `norm_features` is an array\n",
                "containing the normalized NMF features as rows. The names of the musical\n",
                "artists are available as the list `artist_names`.\n",
                "\n",
                "**Instructions**\n",
                "\n",
                "- Import `pandas` as `pd`.\n",
                "- Create a DataFrame `df` from `norm_features`, using `artist_names` as\n",
                "  an index.\n",
                "- Use the `.loc[]` accessor of `df` to select the row of\n",
                "  `'Bruce Springsteen'`. Assign the result to `artist`.\n",
                "- Apply the `.dot()` method of `df` to `artist` to calculate the dot\n",
                "  product of every row with `artist`. Save the result as `similarities`.\n",
                "- Print the result of the `.nlargest()` method of `similarities` to\n",
                "  display the artists most similar to `'Bruce Springsteen'`.\n",
                "\n",
                "**Answer**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Create a DataFrame: df\n",
                "df = pd.DataFrame(norm_features, index=artist_names)\n",
                "\n",
                "# Select row of 'Bruce Springsteen': artist\n",
                "artist = df.loc['Bruce Springsteen']\n",
                "\n",
                "# Compute cosine similarities: similarities\n",
                "similarities = df.dot(artist)\n",
                "\n",
                "# Display those with highest cosine similarity\n",
                "print(similarities.nlargest())\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
